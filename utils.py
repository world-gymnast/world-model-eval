import numpy as np
from PIL import Image
import torch
import base64
import re
import cv2
from openai import OpenAI
import os

def rescale_bridge_action(
    a,
    wv_lo=-0.05,
    wv_hi=+0.05,
    wv_post_scale_max=+1.75,
    wv_post_scale_min=-1.75,
    rd_lo=-0.25,
    rd_hi=+0.25,
    rd_post_scale_max=+1.4,
    rd_post_scale_min=-1.4):
    """
    Rescale Bridge (WidowX) action to the ranges expected by the world model.
    We need to call this function on the unnormalized action values returned by the policy.
    """
    # rescale end effector
    a[:3] = (a[:3] - wv_lo) / (wv_hi - wv_lo) * (
        wv_post_scale_max - wv_post_scale_min
    ) + wv_post_scale_min
    a[:3] = torch.clamp(a[:3], wv_post_scale_min, wv_post_scale_max)
    # rescale joint rotations
    a[3:6] = (a[3:6] - rd_lo) / (rd_hi - rd_lo) * (
        rd_post_scale_max - rd_post_scale_min
    ) + rd_post_scale_min
    a[3:6] = torch.clamp(a[3:6], rd_post_scale_min, rd_post_scale_max)
    # threshold the gripper
    a[6] = torch.where(a[6] > 0.8, -1.0, +1.0)
    return a

def encode_video(video, stride=20):
    frames, idx = [], 0
    for idx, frame in enumerate(video):
        if idx % stride == 0:
            if (frame == 0).all():
                break
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            _, buf = cv2.imencode(".jpg", frame)
            frames.append(base64.b64encode(buf).decode())
    return frames

def evaluate(scores):
    partial = scores[:, 0]  # First subtask scores
    complete = scores[:, 1]  # Second subtask scores
    print(f"Partial completion mean score: {np.round(100*np.mean(partial))=}")
    print(f"Partial completion STE: {np.round(100*np.std(partial) / len(partial)**0.5)=}")
    print(f"Completion mean score: {np.round(100*np.mean(complete))=}")
    print(f"Completion STE: {np.round(100*np.std(complete) / len(complete)**0.5)=}")


def predict(video, task, n=5):
    frames = encode_video(video)
    prompt = f"""
Here is a sequence of frames from a robot policy which has been rolled out in a video-generation-based world model. I need your help determining whether the policy is successful. How successfully does the robot complete the following task?
Task: {task["instruction"]}
Subtask 1: {task["subtasks"][0]}
Subtask 2: {task["subtasks"][1]}

For each subtask, give the model a score of 0 (fail) or 1 (success). Explain your reasoning. Finally, output two lines in the following format:
"Subtask 1: <0 or 1>"
"Subtask 2: <0 or 1>"

Note: Since this video was generated by a video prediction model (conditioned on robot actions), it may contain some artifacts due to the video model capacity.
Note: For the final outputs, make sure to follow the "Subtask i: <score>" format exactly so the score can be parsed out using a regex.
""".strip()
    client = OpenAI()
    messages = [
        {
            "role": "user",
            "content": [prompt, *[{"image": f, "resize": 256} for f in frames]],
        }
    ]
    # print(prompt)
    res = client.chat.completions.create(model="gpt-4o", messages=messages, n=n)
    votes = [0, 0]
    parsed = 0
    for i, choice in enumerate(res.choices):
        content = choice.message.content or ""
        # print(f"\n--- Completion {i} ---\n{content}\n")
        m1 = re.search(r"Subtask\s*1:\s*([01])", content)
        m2 = re.search(r"Subtask\s*2:\s*([01])", content)
        if m1 and m2:
            votes[0] += int(m1.group(1))
            votes[1] += int(m2.group(1))
            parsed += 1
        else:
            print("Warning: parse failure on this completion.")
    if parsed == 0:
        print("No valid parses; returning (False, False).")
        return False, False
    sub1 = votes[0] > parsed / 2.0
    sub2 = votes[1] > parsed / 2.0
    print(f"Parsed {parsed}/{n}. Votes: S1={votes[0]}, S2={votes[1]} -> ({sub1}, {sub2})")
    return sub1, sub2

def load_tasks(root):
    for file in os.listdir(root):
        if file.endswith(".png"):
            base = os.path.splitext(file)[0]
            yield os.path.join(root, base)

TASKS = {
    "/vast/as20482/data/bridge/robot_evaluation/put_carrot_on_plate/": {
        "instruction": "put carrot on plate",
        "subtasks": ["Pick up the carrot.", "Place the carrot on the plate."],
    },
    "/vast/as20482/data/bridge/robot_evaluation/put_eggplant_into_pot_or_pan/": {
        "instruction": "put eggplant into pot or pan",
        "subtasks": ["Pick up the eggplant.", "Place the eggplant into the pot or pan."],
    },
    "/vast/as20482/data/bridge/robot_evaluation/close_microwave/": {
        "instruction": "close microwave",
        "subtasks": ["Push the microwave door.", "Close the microwave door completely."],
    },
    "/vast/as20482/data/bridge/robot_evaluation/stack_blocks/": {
        "instruction": "stack blocks",
        "subtasks": ["Pick up a block.", "Place a block on top of another block."],
    },
    "/vast/as20482/data/bridge/robot_evaluation/close_oven/": {
        "instruction": "close oven",
        "subtasks": ["Push the oven door.", "Close the oven door completely."],
    },
    "/vast/as20482/data/bridge/robot_evaluation/open_microwave/": {
        "instruction": "open the microwave",
        "subtasks": ["Grab microwave handle.", "Pull microwave open."],
    },
    "/vast/as20482/data/bridge/robot_evaluation/sweep_into_pile/": {
        "instruction": "sweep into pile",
        "subtasks": ["Grab the beam.", "Use the beam to sweep into pile."],
    },
    "/vast/as20482/data/bridge/robot_evaluation/fold_cloth/": {
        "instruction": "fold cloth",
        "subtasks": ["Pick up one end of the cloth.", "Fold the cloth over."],
    },
    "/vast/as20482/data/bridge/robot_evaluation/open_oven/": {
        "instruction": "open oven",
        "subtasks": ["Grab oven handle.", "Pull oven open."],
    },
}